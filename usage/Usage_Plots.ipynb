{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf6lO8b24RQz"
   },
   "source": [
    "# This code generates the usage plots for the Symbolic reasoning model and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reproducibility & plotting config\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "FIG_DIR = Path(\"figures\")\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 300,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"figure.figsize\": (6, 4),\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 11,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"legend.fontsize\": 9,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "def savefig(name: str, tight=True):\n",
    "    \"\"\"Save figure to both PNG and PDF in FIG_DIR.\"\"\"\n",
    "    path_png = FIG_DIR / f\"{name}.png\"\n",
    "    path_pdf = FIG_DIR / f\"{name}.pdf\"\n",
    "    if tight:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path_png, bbox_inches=\"tight\")\n",
    "    plt.savefig(path_pdf, bbox_inches=\"tight\")\n",
    "    print(f\"Saved: {path_png} and {path_pdf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sebbPOc44IW"
   },
   "source": [
    "# Rules based model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "339dd233-94a9-4569-a7f1-5e6b5cd656f2"
   },
   "source": [
    "#### Necessary package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d857cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wordfreq\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# Measure memory before loading the model\n",
    "mem_before = process.memory_info().rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "852ea2bd-4fe0-489d-834b-1a5aa6a714b3"
   },
   "source": [
    "#### Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('gdrive/My Drive/Colab Notebooks/clinical_training.csv', index_col=0)\n",
    "df_test = pd.read_csv('gdrive/My Drive/Colab Notebooks/clinical_testing.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e603d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_virtassist = df_train[df_train['dataset'] == 'virtassist']\n",
    "df_test_virtassist = df_test[df_test['dataset'] == 'virtassist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791940fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_virtscribe = df_train[df_train['dataset'] == 'virtscribe']\n",
    "df_test_virtscribe = df_test[df_test['dataset'] == 'virtscribe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e71986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_aci = df_train[df_train['dataset'] == 'aci']\n",
    "df_test_aci = df_test[df_test['dataset'] == 'aci']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60aafd7b-4aec-4419-b31c-7a69e179eea2"
   },
   "source": [
    "#### Global method definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col : name of column in dataframe that you want to interrogate for keywords\n",
    "# indicators : list of keyword arguments\n",
    "# df : dataframe of interest\n",
    "\n",
    "# kwarg_counter : returns (A) t_list : a list of row indicies from the dataframe that do not contain any keywords from indicators\n",
    "#                         (B) t_dict : a list of the frequency of occurence of each word in indicators within the dataframe's column\n",
    "#                         (C) cc : the reading frame of the doctor's passage of speech that contains a keyword specified in \"indicators\"\n",
    "def kwarg_counter(col, indicators, df=df_train):\n",
    "\n",
    "    # t_ind : this index\n",
    "    # t_list : this list of indices\n",
    "    # t_dict : dictionary containing the indicators and their frequency in df[col]\n",
    "    t_ind = 0\n",
    "    t_list = []\n",
    "    t_dict = dict()\n",
    "\n",
    "    # initialize the dict\n",
    "    for i in indicators:\n",
    "        t_dict[i] = 0\n",
    "    t_dict['other'] = 0\n",
    "\n",
    "    # cc : contains chief complaint reading frames\n",
    "    cc = dict()\n",
    "\n",
    "    # for each row in df[col]:\n",
    "    #      iterate through the list of indicators\n",
    "    #           if one indicator in the list of indicators is present among the first 400 characters of the dialogue,\n",
    "    #                 increment t_dict[indicator] and add the whole passage of text to cc (reading frame).\n",
    "    for instance in df[col].values:\n",
    "        t = 0\n",
    "        cc_members = []\n",
    "        for ind in indicators:\n",
    "            if ind in instance[0:400]:\n",
    "                cc_members.append(instance[instance.index(ind) + len(ind):].split('\\n')[0][0:-1])\n",
    "                t_dict[ind] += 1\n",
    "                t+=1\n",
    "        if t == 0:\n",
    "            t_list.append(t_ind)\n",
    "            t_dict['other'] += 1\n",
    "        cc[t_ind] = cc_members\n",
    "        t_ind+=1\n",
    "\n",
    "    return t_list, t_dict, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fcb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_between_ranges : UNUSED method now, but originally intended to observe the frequency of keywords between different ranges\n",
    "def count_between_ranges(list_ref, list_pos):\n",
    "\n",
    "    # sort list_pos to avoid mismatched ranges\n",
    "    list_pos.sort()\n",
    "\n",
    "    # initialize a list to store the counts for each range\n",
    "    counts = []\n",
    "\n",
    "    # loop through each pair of consecutive values in list_pos\n",
    "    for i in range(len(list_pos) - 1):\n",
    "        start = list_pos[i]\n",
    "        end = list_pos[i + 1]\n",
    "\n",
    "        # count the number of elements in list_ref that fall within the range [start, end]\n",
    "        count = sum(start < x < end for x in list_ref)\n",
    "        counts.append(count)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given: A string containing a patient-physician conversation, stored in 'dialogue' column of dataframe\n",
    "# returns: A string containing exclusively the doctor's speech, denoted by [doctor] in the text\n",
    "def doctor_from_dialogue(dialogue):\n",
    "\n",
    "    exchanges = dialogue.split('\\n')\n",
    "    doc_sentences = []\n",
    "\n",
    "    for e in exchanges:\n",
    "        if '[doctor]' in e:\n",
    "            doc_sentences.append(e)\n",
    "\n",
    "    return doc_sentences\n",
    "\n",
    "lo_convo = []\n",
    "for convo in df_train['dialogue'].values:\n",
    "    ds_convo = doctor_from_dialogue(convo)\n",
    "    lo_convo.append('\\n'.join(ds_convo))\n",
    "\n",
    "df_train['doc_dialogues'] = lo_convo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3a3dd7c-110d-4868-962b-68d82c70435b"
   },
   "source": [
    "#### Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "295f3b7e-ea68-488b-bff9-b605f44b82a0"
   },
   "source": [
    "##### Are the outputs notes from genAI methods standardized? Mostly, yes. 88% of notes outputs in the training set include the words \"CHIEF COMPLAINT\" and 5.97% exclude a chief complaints section entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_missing_CC, dict_of_counts, cc_reading_frames_from_notes = kwarg_counter('note', ['CHIEF COMPLAINT\\n\\n', 'CC:\\n\\n'])\n",
    "dict_of_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af522acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_missing_CC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8e13b7d-1c83-45ed-b359-5e77055f646e",
    "scrolled": true
   },
   "source": [
    "#### Are there words used consistently in dialogue to indicate a chief complaint? Partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list_dialogue_1, t_dict_dialogue_1, cc_from_dialogue_1 = kwarg_counter('dialogue', ['present', 'eval'])\n",
    "t_dict_dialogue_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72586ccf-1a82-446d-bc27-eebf4e65ce04"
   },
   "source": [
    "##### What other words can be included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d76338",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list_dialogue_2, t_dict_dialogue_2, cc_from_dialogue_2 = kwarg_counter('dialogue', ['present', 'eval', 'here for', 'here with', 'for'])\n",
    "t_dict_dialogue_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e2d7328-f9c4-4b96-ada6-226018950593"
   },
   "source": [
    "##### There is full coverage in the dataset using these five keywords! Essential to make sure that \"for\" is within a reading frame of a chief complaint (which is an analysis we have conducted separately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ef70c1a-9b60-4953-b75c-3c13c5b9036c"
   },
   "source": [
    "### Let's look at just the virtassist subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_virtassist.head()\n",
    "#df_test_virtassist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63567432-1d73-4def-9fd7-e63b22a35eb2"
   },
   "source": [
    "#### Are there keywords in virtassist that indicate chief complaint? Looks like it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76582af",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtassist_inds, virtassist_dict, virtassist_frames = kwarg_counter('dialogue', ['present', 'eval', 'here for', 'here with'], df_train_virtassist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtassist_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d1e913c-6db2-4421-9591-7307dc36f4c1"
   },
   "source": [
    "#### Let's clean the reading frames from the keywords identified above, and turn each reading frame into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading_frames : a list of reading frames\n",
    "\n",
    "# reading_frame_cleaner : splits the reading frame to only contain the first sentence of the dialogue.\n",
    "# Returns a list of cleaned reading frames\n",
    "def reading_frame_cleaner(reading_frames):\n",
    "\n",
    "    # list of \"very accurate\" chief complaints (it actually stood for virtassist; too lazy to change now)\n",
    "    va_cc = []\n",
    "\n",
    "    # for a frame in reading frames,\n",
    "    #     if there is something in the reading frame,\n",
    "    #          append everything between the first english character and the period to va_cc\n",
    "    #     otherwise, append NULL\n",
    "    for t in reading_frames:\n",
    "        if reading_frames[t] != []:\n",
    "            first_part = reading_frames[t][0].split('.')[0]\n",
    "            if \" \" in first_part:\n",
    "                va_cc.append(first_part.split(\" \", 1)[1])\n",
    "            else:\n",
    "                va_cc.append('NULL')\n",
    "        else:\n",
    "            va_cc.append('NULL')\n",
    "\n",
    "    return va_cc\n",
    "\n",
    "#va_cc = reading_frame_cleaner(virtassist_frames)\n",
    "#va_cc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ceb00c8-6be7-488e-ba83-bfd483530195"
   },
   "source": [
    "##### Now I can tokenize words in a reading frame to compute the freqencies of each token in the english language using the wordfreq library: calculating the zipf (log-10 occurrences per million words) for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45145ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo_frames : list of cleaned reading frames\n",
    "\n",
    "# frame_tokenizer : splits a reading frame into individual words\n",
    "def frame_tokenizer(lo_frames):\n",
    "\n",
    "    lo_tkn = []\n",
    "\n",
    "    # for each reading frame, split the frame by words (space character) and append to lo_tkn\n",
    "    for phrase in lo_frames:\n",
    "        lo_tkn.append(phrase.split(' ')[0:-1])\n",
    "\n",
    "    return lo_tkn\n",
    "\n",
    "#lo_tkn = frame_tokenizer(va_cc)\n",
    "#lo_tkn[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56625f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo_tkn : a list containing a list of tokens (words)\n",
    "\n",
    "# freq_calc : calculate the zipf's law word frequency using a dictionary of 1 million english words.\n",
    "def freq_calc(lo_tkn):\n",
    "\n",
    "    lo_freqs = []\n",
    "\n",
    "    # for a tokenized reading frame,\n",
    "    #     for each token,\n",
    "    #          fetch the zipf frequency and append to series_freqs\n",
    "    # append all the series freqs (corresponding to reading frames) together in lo_freqs\n",
    "    for tkn_series in lo_tkn:\n",
    "        series_freqs = []\n",
    "        for tkn in tkn_series:\n",
    "            series_freqs.append(wordfreq.zipf_frequency(tkn, 'en', wordlist='small'))\n",
    "        lo_freqs.append(series_freqs)\n",
    "    return lo_freqs\n",
    "\n",
    "#lo_freqs = freq_calc(lo_tkn)\n",
    "#lo_freqs[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb62bb2c-c5d6-4ba6-8dd8-873fe69c0b5a"
   },
   "source": [
    "#### Can we identify medical breaks in the list of tokens based on a significant change in frequency score? Let's try with a delta of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a36f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo_tkn and lo_freqs are previously defined\n",
    "\n",
    "# frequency_parser : identifies a breakpoint where the change in\n",
    "def frequency_parser(lo_tkn, lo_freqs):\n",
    "\n",
    "    freq_subsetter_idx = 0\n",
    "    cc_fisher = []\n",
    "    while freq_subsetter_idx < len(lo_tkn):\n",
    "\n",
    "        word_list = lo_tkn[freq_subsetter_idx]\n",
    "        frequency_list = lo_freqs[freq_subsetter_idx]\n",
    "\n",
    "        # Initialize variable to store the index after the drop\n",
    "        drop_index = None\n",
    "\n",
    "        # Find the index where the drop in frequency is greater than 1.0\n",
    "        for i in range(1, len(frequency_list)):\n",
    "            if frequency_list[i-1] - frequency_list[i] > 1.0:\n",
    "                drop_index = i\n",
    "                break\n",
    "\n",
    "        # Take subset of word_list from the drop index onward\n",
    "        subset_word_list = word_list[drop_index:] if drop_index is not None else []\n",
    "        if len(subset_word_list) == 0:\n",
    "            subset_word_list.append('')\n",
    "\n",
    "        if subset_word_list[0] == 'pain':\n",
    "            subset_word_list.insert(0, word_list[word_list.index('pain')-1])\n",
    "\n",
    "        cc_final = ' '.join(subset_word_list)\n",
    "        if len(cc_final) > 50:\n",
    "            cc_final = cc_final[0:50]\n",
    "        ## OUTPUT LABEL -- should contain the chief complaint\n",
    "        cc_fisher.append(cc_final)\n",
    "\n",
    "        freq_subsetter_idx+=1\n",
    "\n",
    "    return cc_fisher\n",
    "\n",
    "#cc_fisher = frequency_parser(lo_tkn, lo_freqs)\n",
    "#cc_fisher[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51072e5d-b425-4d89-9774-f0b5efc82734"
   },
   "source": [
    "##### Now let's compare the extracted chief complaints in the virtassist training set with the ground truth chief complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14283f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_virtassist['chief_complaint'].values[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80cffd75-be2e-4d11-bdec-ac91ab4cecc9"
   },
   "source": [
    "### Putting this altogether for chief complaint extraction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828851e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chief_complaint_from_dialogue(df, word_list = ['present', 'eval', 'here for', 'here with', 'for']):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    noncomplacent_inds, kwarg_occurence_count, reading_frames = kwarg_counter('dialogue', word_list, df)\n",
    "    lo_frames = reading_frame_cleaner(reading_frames)\n",
    "    lo_tkn = frame_tokenizer(lo_frames)\n",
    "    lo_freqs = freq_calc(lo_tkn)\n",
    "    cc_fisher = frequency_parser(lo_tkn, lo_freqs)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time taken: \" + str(end - start) + \" seconds\")\n",
    "\n",
    "    return cc_fisher, [lo_tkn, lo_freqs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd95edcb-34a9-4dab-bb76-15ed67c05c2b"
   },
   "source": [
    "##### Check that the code runs appropriately on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b64c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_a, temp_b = chief_complaint_from_dialogue(df_train)\n",
    "temp_a[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUjLTKJFvzCt"
   },
   "source": [
    "### Creating memory and time usage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "def chief_complaint_from_dialogue_stats(df, word_list=['present', 'eval', 'here for', 'here with', 'for']):\n",
    "    start = time.time()\n",
    "    tracemalloc.start()  # Start tracking memory\n",
    "\n",
    "    # Run your analysis pipeline\n",
    "    noncomplacent_inds, kwarg_occurence_count, reading_frames = kwarg_counter('dialogue', word_list, df)\n",
    "    lo_frames = reading_frame_cleaner(reading_frames)\n",
    "    lo_tkn = frame_tokenizer(lo_frames)\n",
    "    lo_freqs = freq_calc(lo_tkn)\n",
    "    cc_fisher = frequency_parser(lo_tkn, lo_freqs)\n",
    "\n",
    "    current, peak = tracemalloc.get_traced_memory()  # Get memory in bytes\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    peak_memory_MB = peak / 1024 / 1024  # Convert bytes to megabytes\n",
    "\n",
    "    return total_time, peak_memory_MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8211494",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_runs_srm=[]\n",
    "memory_runs_srm=[]\n",
    "for i in range(200):\n",
    "  time_i, memory_i=chief_complaint_from_dialogue_stats(df_test.sample(frac=1, random_state=i),\n",
    "                              word_list= ['present', 'eval', 'here for', 'here with', 'with', 'for'])\n",
    "  time_runs_srm.append(time_i)\n",
    "  memory_runs_srm.append(memory_i)\n",
    "\n",
    "print(np.mean(time_runs_srm) * 1000,' ms')\n",
    "\n",
    "std_dev = np.std(time_runs_srm, ddof=1)\n",
    "n = np.size(time_runs_srm)\n",
    "sem = std_dev / np.sqrt(n)\n",
    "\n",
    "print(sem * 1000,' ms')\n",
    "#print(np.mean(memory_runs_srm),' MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06537f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_after = process.memory_info().rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_used_MB = (mem_after - mem_before) / 1024 / 1024\n",
    "memory_used_MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk1aST9F6PlF"
   },
   "source": [
    "# LLMs\n",
    "Instead of re-training or fine-tuning the model on our clinical documentation dataset, we evaluated its zero-shot capability to generate the chief complaints based on the dialouge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBJUkNfwKemr"
   },
   "source": [
    "## Load necessary libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set groq API\n",
    "api_key = \"YOUR_GROQ_API_KEY\"\n",
    "client = groq.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiMo_t8XK8uj"
   },
   "source": [
    "## Start experiment-specific loading files and function\n",
    "This is when I start recording memory usage\n",
    "\n",
    "Model run limits: https://console.groq.com/docs/rate-limits\n",
    "\n",
    "to-do: This needs to be updated to run more than 1 iteration per run, need developer API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ff0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_LLM_performance(model_type, prompt, df_test):\n",
    "  start = time.time()\n",
    "  # get dialouge and chief complaint from test data\n",
    "  inputs = np.array(df_test['dialogue'].tolist())\n",
    "  labels = np.array(df_test['chief_complaint'].tolist())\n",
    "  # holder for 11 category labels, run-time, and LLM prediction\n",
    "  list_category_labels = []\n",
    "  list_category_preds = []\n",
    "  total_prompt_tokens = 0\n",
    "  total_completion_tokens = 0\n",
    "\n",
    "  # Loop through test set and use LLM to get the predictions\n",
    "  for index in range(df_test.shape[0]):\n",
    "      transcription = inputs[index]\n",
    "      label = labels[index]\n",
    "      category_label = labelMap.get(label.lower().rstrip(), 'others')\n",
    "\n",
    "      # call groq api\n",
    "      chat_completion = client.chat.completions.create(\n",
    "          messages=[\n",
    "              {\"role\": \"system\", \"content\": prompt},\n",
    "              {\"role\": \"user\", \"content\": transcription}\n",
    "          ],\n",
    "          model=model_type,\n",
    "      )\n",
    "\n",
    "      # get the response from LLM\n",
    "      pred = chat_completion.choices[0].message.content\n",
    "      usage = chat_completion.usage  # Track token usage\n",
    "      total_prompt_tokens += usage.prompt_tokens\n",
    "      total_completion_tokens += usage.completion_tokens\n",
    "\n",
    "      list_category_labels.append(category_label)\n",
    "      list_category_preds.append(pred)\n",
    "\n",
    "  end = time.time()\n",
    "  total_time = end - start\n",
    "\n",
    "  total_tokens = total_prompt_tokens + total_completion_tokens\n",
    "\n",
    "  # https://groq.com/pricing\n",
    "  if model_type == \"llama-3.3-70b-versatile\":\n",
    "    cost = (total_prompt_tokens / 1e6 * 0.59) + (total_completion_tokens / 1e6 * 0.79)\n",
    "  elif model_type == \"llama3-70b-8192\":\n",
    "    cost = (total_prompt_tokens / 1e6 * 0.59) + (total_completion_tokens / 1e6 * 0.79)\n",
    "  elif model_type == \"gemma2-9b-it\":\n",
    "    cost = (total_prompt_tokens / 1e6 * 0.20) + (total_completion_tokens / 1e6 * 0.20)\n",
    "  else:\n",
    "    print(\"Models don't match, cost can't be calculated\")\n",
    "\n",
    "  return total_time, total_tokens, cost, total_prompt_tokens, total_completion_tokens\n",
    "\n",
    "\n",
    "def run_model_prompt(model_type_list, prompt_list, prompt_names_list):\n",
    "\n",
    "  process = psutil.Process(os.getpid())\n",
    "  # Measure memory before loading the model\n",
    "  mem_before = process.memory_info().rss\n",
    "\n",
    "  #Load data\n",
    "  train_df = pd.read_csv('gdrive/My Drive/Colab Notebooks/clinical_training.csv', index_col=0)\n",
    "  test_df = pd.read_csv('gdrive/My Drive/Colab Notebooks/clinical_testing.csv', index_col=0)\n",
    "\n",
    "  #Set up variables\n",
    "  time_runs_list=[]\n",
    "  memory_used_list=[]\n",
    "  total_input_tokens_list=[]\n",
    "  total_output_tokens_list=[]\n",
    "  total_tokens_list=[]\n",
    "  total_costs_list=[]\n",
    "\n",
    "  for run in np.arange(len(model_type_list)):\n",
    "    model_type = model_type_list[run]\n",
    "    system_promp = prompt_list[run]\n",
    "    prompt_name = prompt_names_list[run]\n",
    "\n",
    "    print(f\"Run {run+1}: Model {model_type} on {prompt_name}\")\n",
    "\n",
    "    time_runs=[]\n",
    "    total_input_tokens=[]\n",
    "    total_output_tokens=[]\n",
    "    total_tokens=[]\n",
    "    total_costs=[]\n",
    "    for i in range(30):\n",
    "      time_i, tokens_i, cost_i, input_token_i, output_token_i =measure_LLM_performance(model_type, system_promp, test_df.sample(frac=1, random_state=i))\n",
    "      time_runs.append(time_i)\n",
    "      total_tokens.append(tokens_i)\n",
    "      total_costs.append(cost_i)\n",
    "      total_input_tokens.append(input_token_i)\n",
    "      total_output_tokens.append(output_token_i)\n",
    "\n",
    "\n",
    "    # Save each run into these lists\n",
    "    time_runs_list.append(time_runs)\n",
    "    total_input_tokens_list.append(total_input_tokens)\n",
    "    total_output_tokens_list.append(total_output_tokens)\n",
    "    total_tokens_list.append(total_tokens)\n",
    "    total_costs_list.append(total_costs)\n",
    "\n",
    "    print(np.mean(time_runs) * 1000,' ms')\n",
    "\n",
    "\n",
    "    std_dev = np.std(time_runs, ddof=1)\n",
    "    n = np.size(time_runs)\n",
    "    sem = std_dev / np.sqrt(n)\n",
    "\n",
    "    print(sem * 1000,' ms')\n",
    "\n",
    "    mem_after = process.memory_info().rss\n",
    "\n",
    "    memory_used_MB = (mem_after - mem_before) / 1024 / 1024\n",
    "    print(memory_used_MB, 'MB')\n",
    "    memory_used_list.append(memory_used_MB)\n",
    "\n",
    "    std_dev_cost = np.std(total_costs, ddof=1)\n",
    "    n_cost = np.size(total_costs)\n",
    "    sem_cost = std_dev_cost / np.sqrt(n_cost)\n",
    "\n",
    "    print(np.mean(total_tokens), 'total tokens')\n",
    "    print('$',np.mean(total_costs))\n",
    "    print('$',sem_cost)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print('-----------------------------------')\n",
    "\n",
    "  return time_runs_list, memory_used_list, total_input_tokens_list, total_output_tokens_list, total_tokens_list, total_costs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model type and content for system\n",
    "#model_type = \"llama-3.3-70b-versatile\"\n",
    "#model_type = \"llama3-70b-8192\" # only for estimating when other model is overused\n",
    "#model_type = \"gemma2-9b-it\"\n",
    "# prompt A\n",
    "system_promp_A =  \"\"\"\n",
    "You are a doctor. Your task is to identify the patient's chief complaint based on the following dialogue between a doctor and a patient.\n",
    "Please select a direct quotation from the dialogue which best identifies the chief complaint. You may use ellipses \"(...)\" to skip irrelevant dialogue\n",
    "within the detected chief complaint. Simply provide your best assessment. No explanation is needed. \"\"\"\n",
    "system_promp_A = system_promp_A.replace(\"\\n\", \" \")\n",
    "\n",
    "# Prompt B\n",
    "\n",
    "system_promp_B =  \"\"\"\n",
    "You are a doctor. Your task is to identify the patient's chief complaint based on the following dialogue between a doctor and a patient.\n",
    "Please select a direct quotation from the dialogue which best identifies the chief complaint. You may use ellipses \"(...)\" to skip irrelevant dialogue\n",
    "within the detected chief complaint. Simply provide your best assessment. No explanation is needed. Be concise; detect 10 or fewer words to summarize the chief complaint. \"\"\"\n",
    "system_promp_B = system_promp_B.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a113457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_list=[\"gemma2-9b-it\",\"gemma2-9b-it\",\"llama-3.3-70b-versatile\",\"llama-3.3-70b-versatile\"]\n",
    "prompt_list=[system_promp_A,system_promp_B,system_promp_A,system_promp_B]\n",
    "prompt_names_list=['Verbose', 'Succinct', 'Verbose', 'Succinct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up variables\n",
    "time_runs_list=[]\n",
    "memory_used_list=[]\n",
    "total_input_tokens_list=[]\n",
    "total_output_tokens_list=[]\n",
    "total_tokens_list=[]\n",
    "total_costs_list=[]\n",
    "\n",
    "time_runs_list, memory_used_list, total_input_tokens_list, total_output_tokens_list, total_tokens_list, total_costs_list = run_model_prompt(model_type_list, prompt_list, prompt_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fccab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzv14qmVMV5C"
   },
   "source": [
    "## Plot the Data\n",
    "Need to rework this to adjust for the new lists of outputs in run_model_prompt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46260928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Symbolic Model', 'Gemma-9b-succinct', 'Gemma-9b-Verbose', 'Llama-3.3-70b-succint', 'Llama-3.3-70b-Verbose']\n",
    "run_time_ms = [4.091320037841797, 72480.13353347778,71777.21548080444,101445.04475593567,146168.6553955078]  # Time in milliseconds\n",
    "peak_memory_MB = [21.3046875,0.47265625,0.50390625,0.46875,0.53515625]   # Memory usage in megabytes\n",
    "\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.35  # width of the bars\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for run time\n",
    "bars1 = ax1.bar(x - width/2, run_time_ms, width, label='Run Time (ms)', color='#4878d0', alpha=0.8)\n",
    "\n",
    "# Second Y-axis for memory\n",
    "ax2 = ax1.twinx()\n",
    "bars2 = ax2.bar(x + width/2, peak_memory_MB, width, label='Peak Memory (MB)', color='#d65f5f', alpha=0.8)\n",
    "\n",
    "# Labels and titles\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Run Time (ms)', color='#4878d0')\n",
    "ax2.set_ylabel('Peak Memory (MB)', color='#d65f5f')\n",
    "ax1.set_title('Run Time and Memory Usage per Model')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Symbolic Model', 'Gemma-9b-succinct', 'Gemma-9b-Verbose', 'Llama-3.3-70b-succint', 'Llama-3.3-70b-Verbose']\n",
    "run_time_ms = [4.09, 72480.13, 71777.22, 101445.04, 146168.66]  # Time in milliseconds\n",
    "peak_memory_MB = [21.30, 0.47, 0.50, 0.47, 0.54]   # Memory usage in megabytes\n",
    "\n",
    "# Example standard deviation or error margins (replace with real values!)\n",
    "run_time_std = [0.5, 1500, 1600, 2000, 2500]\n",
    "peak_memory_std = [0.1, 0.02, 0.03, 0.02, 0.04]\n",
    "\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, run_time_ms, width,\n",
    "                yerr=run_time_std, capsize=5,\n",
    "                label='Run Time (ms)', color='#4878d0', alpha=0.8)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "bars2 = ax2.bar(x + width/2, peak_memory_MB, width,\n",
    "                yerr=peak_memory_std, capsize=5,\n",
    "                label='Peak Memory (MB)', color='#d65f5f', alpha=0.8)\n",
    "\n",
    "\n",
    "# Log scale\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Axis labels and ticks\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Run Time (ms)')\n",
    "ax2.set_ylabel('Peak Memory (MB)')\n",
    "ax1.set_title('Run Time and Memory Usage per Model')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=15, ha='right')\n",
    "\n",
    "# Legend\n",
    "bars = [bars1[0], bars2[0]]\n",
    "labels = ['Run Time (ms)', 'Peak Memory (MB)']\n",
    "ax1.legend(bars, labels, loc='upper left')\n",
    "\n",
    "# Significance logic\n",
    "def get_sig_star(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    return 'ns'\n",
    "\n",
    "# # --- 1. Run Time p-values (on ax1) ---\n",
    "# rt_comparisons = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "# rt_p_values = [0.01, 0.005, 0.0005, 0.0001]\n",
    "# v_offset_rt = 4000  # vertical spacing between each line\n",
    "\n",
    "# for idx, ((i, j), p) in enumerate(zip(rt_comparisons, rt_p_values)):\n",
    "#     base_y = max(run_time_ms[i], run_time_ms[j]) + 5000\n",
    "#     h = 3000 + idx * v_offset_rt\n",
    "#     ax1.plot([x[i] - width/2, x[i] - width/2, x[j] - width/2, x[j] - width/2],\n",
    "#              [base_y + h, base_y + h + 1000, base_y + h + 1000, base_y + h],\n",
    "#              lw=1.5, color='black')\n",
    "#     ax1.text((x[i] + x[j]) / 2 - width/2, base_y + h + 1100,\n",
    "#              get_sig_star(p), ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# --- 2. Peak Memory p-values (on ax2) ---\n",
    "pm_comparisons = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "pm_p_values = [0.03, 0.02, 0.04, 0.045]\n",
    "v_offset_pm = 0.8  # vertical spacing between each line\n",
    "\n",
    "for idx, ((i, j), p) in enumerate(zip(pm_comparisons, pm_p_values)):\n",
    "    base_y = max(peak_memory_MB[i], peak_memory_MB[j]) + 0.5\n",
    "    h = 0.5 + idx * v_offset_pm\n",
    "    ax2.plot([x[i] + width/2, x[i] + width/2, x[j] + width/2, x[j] + width/2],\n",
    "             [base_y + h, base_y + h + 0.2, base_y + h + 0.2, base_y + h],\n",
    "             lw=1.5, color='black')\n",
    "    ax2.text((x[i] + x[j]) / 2 + width/2, base_y + h + 0.25,\n",
    "             get_sig_star(p), ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Symbolic Model', 'Gemma-9b-succinct', 'Gemma-9b-Verbose', 'Llama-3.3-70b-succint', 'Llama-3.3-70b-Verbose']\n",
    "run_time_ms = [5.74, 65408.52, 63359.42, 5777.58, 6720.83]\n",
    "peak_memory_MB = [21.30, 6.625, 6.109, 6.882, 6.882]\n",
    "\n",
    "run_time_std = [0.1, 75.63, 1976.70, 48.26, 58.76]\n",
    "peak_memory_std = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.6\n",
    "\n",
    "def get_sig_star(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    return 'ns'\n",
    "\n",
    "def draw_sig(ax, xi, xj, y, h, label, dx=0.0, lw=1.5):\n",
    "    \"\"\"Draw a significance bracket with optional horizontal offset dx.\"\"\"\n",
    "    x1, x2 = xi + dx, xj + dx\n",
    "    ax.plot([x1, x1, x2, x2],\n",
    "            [y,  y + h, y + h, y],\n",
    "            lw=lw, color='black', clip_on=False)\n",
    "    ax.text((x1 + x2) / 2, y + h * 1.05, label,\n",
    "            ha='center', va='bottom', fontsize=12, color='black', clip_on=False)\n",
    "\n",
    "# --- Plot 1: Run Time (log) — multiplicative spacing ---\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "bars1 = ax1.bar(x, run_time_ms, yerr=run_time_std, capsize=5, color='#4878d0', alpha=0.85)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax1.set_ylabel('Run Time (ms)')\n",
    "ax1.set_title('Run Time per Model')\n",
    "ax1.set_xlabel('Model')\n",
    "\n",
    "rt_comparisons = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "rt_p_values    = [0.01, 0.005, 0.0005, 0.0001]\n",
    "rt_dx = np.linspace(-0.18, 0.18, len(rt_comparisons))\n",
    "\n",
    "# Key knobs for log spacing\n",
    "above_frac   = 0.10   # start each bracket 10% above taller of the pair\n",
    "level_factor = 1.7    # how much higher each successive bracket sits (uniform in log space)\n",
    "h_frac       = 0.12   # bracket height as a fraction of its baseline level\n",
    "\n",
    "y_needed_max = ax1.get_ylim()[1]\n",
    "for idx, ((i, j), p) in enumerate(zip(rt_comparisons, rt_p_values)):\n",
    "    pair_top = max(run_time_ms[i], run_time_ms[j])\n",
    "    # multiplicative level in log space\n",
    "    level = pair_top * (1.0 + above_frac) * (level_factor ** idx)\n",
    "    h = level * h_frac\n",
    "    draw_sig(ax1, x[i], x[j], level, h, get_sig_star(p), dx=rt_dx[idx])\n",
    "    y_needed_max = max(y_needed_max, level + 1.25*h)\n",
    "\n",
    "# ensure headroom on log axis\n",
    "ymin, ymax = ax1.get_ylim()\n",
    "if y_needed_max > ymax:\n",
    "    ax1.set_ylim(ymin, y_needed_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Plot 2: Peak Memory ---\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 8))\n",
    "bars2 = ax2.bar(x, peak_memory_MB, capsize=5, color='#d65f5f', alpha=0.85)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax2.set_ylabel('Peak Memory (MB)')\n",
    "ax2.set_title('Peak Memory Usage per Model')\n",
    "ax2.set_xlabel('Model')\n",
    "\n",
    "pm_comparisons = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "pm_p_values     = [0.03, 0.02, 0.04, 0.045]\n",
    "\n",
    "# Jitters for memory brackets\n",
    "pm_dx = np.linspace(-0.18, 0.18, len(pm_comparisons))\n",
    "\n",
    "v_offset_pm = 0.5   # bracket height\n",
    "gap_pm      = 0.5   # space above tallest of the pair\n",
    "y_needed_max = 0\n",
    "\n",
    "for idx, ((i, j), p) in enumerate(zip(pm_comparisons, pm_p_values)):\n",
    "    base_y = max(peak_memory_MB[i], peak_memory_MB[j]) + gap_pm\n",
    "    h = v_offset_pm + idx * 1.5\n",
    "    draw_sig(ax2, x[i], x[j], base_y, h, get_sig_star(p), dx=pm_dx[idx])\n",
    "    y_needed_max = max(y_needed_max, base_y + h * 1.2)\n",
    "\n",
    "ymin, ymax = ax2.get_ylim()\n",
    "if y_needed_max > ymax:\n",
    "    ax2.set_ylim(ymin, y_needed_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04617097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind_from_stats  # NEW: to compute p-values from summary stats\n",
    "\n",
    "models = ['Symbolic Model', 'Gemma-9b-succinct', 'Gemma-9b-Verbose', 'Llama-3.3-70b-succint', 'Llama-3.3-70b-Verbose']\n",
    "run_time_ms = [5.74, 65408.52, 63359.42, 5777.58, 6720.83]\n",
    "peak_memory_MB = [21.30, 6.625, 6.109, 6.882, 6.882]\n",
    "\n",
    "run_time_std = [0.1, 75.63, 1976.70, 48.26, 58.76]\n",
    "peak_memory_std = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "# --- Sample size per model (given) ---\n",
    "n = 30\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.6\n",
    "\n",
    "def get_sig_star(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    return 'ns'\n",
    "\n",
    "def draw_sig(ax, xi, xj, y, h, label, dx=0.0, lw=1.5):\n",
    "    \"\"\"Draw a significance bracket with optional horizontal offset dx.\"\"\"\n",
    "    x1, x2 = xi + dx, xj + dx\n",
    "    ax.plot([x1, x1, x2, x2],\n",
    "            [y,  y + h, y + h, y],\n",
    "            lw=lw, color='black', clip_on=False)\n",
    "    ax.text((x1 + x2) / 2, y + h * 1.05, label,\n",
    "            ha='center', va='bottom', fontsize=12, color='black', clip_on=False)\n",
    "\n",
    "def safe_std(s):\n",
    "    # Avoid zero-variance crashing Welch's t-test\n",
    "    return s if s > 0 else 1e-9\n",
    "\n",
    "# --- Plot 1: Run Time (log) — multiplicative spacing ---\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "bars1 = ax1.bar(x, run_time_ms, yerr=run_time_std, capsize=5, color='#4878d0', alpha=0.85)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax1.set_ylabel('Run Time (ms)')\n",
    "ax1.set_title('Run Time per Model')\n",
    "ax1.set_xlabel('Model')\n",
    "\n",
    "rt_comparisons = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "rt_dx = np.linspace(-0.18, 0.18, len(rt_comparisons))\n",
    "\n",
    "# Key knobs for log spacing\n",
    "above_frac   = 0.10   # start each bracket 10% above taller of the pair\n",
    "level_factor = 1.7    # how much higher each successive bracket sits (uniform in log space)\n",
    "h_frac       = 0.12   # bracket height as a fraction of its baseline level\n",
    "\n",
    "y_needed_max = ax1.get_ylim()[1]\n",
    "for idx, (i, j) in enumerate(rt_comparisons):\n",
    "    # Compute p-value from means/stds with n=30 using Welch's t-test\n",
    "    _, p = ttest_ind_from_stats(mean1=run_time_ms[i], std1=safe_std(run_time_std[i]), nobs1=n,\n",
    "                                mean2=run_time_ms[j], std2=safe_std(run_time_std[j]), nobs2=n,\n",
    "                                equal_var=False)\n",
    "\n",
    "    pair_top = max(run_time_ms[i], run_time_ms[j])\n",
    "    level = pair_top * (1.0 + above_frac) * (level_factor ** idx)\n",
    "    h = level * h_frac\n",
    "    draw_sig(ax1, x[i], x[j], level, h, get_sig_star(p), dx=rt_dx[idx])\n",
    "    y_needed_max = max(y_needed_max, level + 1.25*h)\n",
    "\n",
    "# ensure headroom on log axis\n",
    "ymin, ymax = ax1.get_ylim()\n",
    "if y_needed_max > ymax:\n",
    "    ax1.set_ylim(ymin, y_needed_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Peak Memory ---\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 8))\n",
    "bars2 = ax2.bar(x, peak_memory_MB, capsize=5, color='#d65f5f', alpha=0.85)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax2.set_ylabel('Peak Memory (MB)')\n",
    "ax2.set_title('Peak Memory Usage per Model')\n",
    "ax2.set_xlabel('Model')\n",
    "\n",
    "pm_comparisons = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "\n",
    "# Jitters for memory brackets\n",
    "pm_dx = np.linspace(-0.18, 0.18, len(pm_comparisons))\n",
    "\n",
    "v_offset_pm = 0.5   # bracket height\n",
    "gap_pm      = 0.5   # space above tallest of the pair\n",
    "y_needed_max = 0\n",
    "\n",
    "for idx, (i, j) in enumerate(pm_comparisons):\n",
    "    # Compute p-value from means/stds with n=30 using Welch's t-test\n",
    "    _, p = ttest_ind_from_stats(mean1=peak_memory_MB[i], std1=safe_std(peak_memory_std[i]), nobs1=n,\n",
    "                                mean2=peak_memory_MB[j], std2=safe_std(peak_memory_std[j]), nobs2=n,\n",
    "                                equal_var=False)\n",
    "\n",
    "    base_y = max(peak_memory_MB[i], peak_memory_MB[j]) + gap_pm\n",
    "    h = v_offset_pm + idx * 1.5\n",
    "    draw_sig(ax2, x[i], x[j], base_y, h, get_sig_star(p), dx=pm_dx[idx])\n",
    "    y_needed_max = max(y_needed_max, base_y + h * 1.2)\n",
    "\n",
    "ymin, ymax = ax2.get_ylim()\n",
    "if y_needed_max > ymax:\n",
    "    ax2.set_ylim(ymin, y_needed_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f536a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "339dd233-94a9-4569-a7f1-5e6b5cd656f2",
    "852ea2bd-4fe0-489d-834b-1a5aa6a714b3",
    "60aafd7b-4aec-4419-b31c-7a69e179eea2",
    "e3a3dd7c-110d-4868-962b-68d82c70435b",
    "295f3b7e-ea68-488b-bff9-b605f44b82a0",
    "c8e13b7d-1c83-45ed-b359-5e77055f646e",
    "3ef70c1a-9b60-4953-b75c-3c13c5b9036c",
    "80cffd75-be2e-4d11-bdec-ac91ab4cecc9"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
